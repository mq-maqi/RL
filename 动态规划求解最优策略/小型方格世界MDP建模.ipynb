{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 状态空间\n",
    "S = [i for i in range(16)]\n",
    "# 行为空间\n",
    "A = [\"n\", \"e\", \"s\", \"w\"]\n",
    "# 行为对状态的改变\n",
    "ds_actions = {\"n\":-4, \"e\":1, \"s\":4, \"w\":-1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "def dynamics(s,a):\n",
    "    \"\"\"\n",
    "    模拟小型方格世界的环境动力学特征\n",
    "    :param s: 表示当前状态0-15\n",
    "    :param a: 表示行为 str in ['n'.'e','s','w']分别表示北、东、南、西\n",
    "    :return: tuple(s_prime, reward, is_end)\n",
    "    s_prime 后续状态\n",
    "    reward 奖励值\n",
    "    is_end 是否进入终止状态\n",
    "    \"\"\"\n",
    "    s_prime = s\n",
    "    # 过滤边界状态\n",
    "    if (s % 4 == 0 and a == \"w\") or (s < 4 and a == \"n\") or ((s+1)%4 == 0 and a == \"e\") or (s > 11 and a == \"s\") or s in [0,15]:\n",
    "        pass\n",
    "    else:\n",
    "        ds = ds_actions[a]\n",
    "        s_prime = s + ds\n",
    "    reward = 0 if s in [0,15] else -1\n",
    "    is_end = True if s in [0,15] else False\n",
    "    return s_prime, reward, is_end\n",
    "\n",
    "def P(s,a,s1):\n",
    "    \"\"\"\n",
    "    状态转移概率函数\n",
    "    :param s:\n",
    "    :param a:\n",
    "    :param s1:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    s_prime,_,_ = dynamics(s,a)\n",
    "    return s1 == s_prime\n",
    "\n",
    "def R(s,a):\n",
    "    _, r, _ = dynamics(s,a)\n",
    "    return r\n",
    "\n",
    "gamma = 1.0\n",
    "MDP = S, A, R, P, gamma"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# 均一随机策略（只需要知道行为空间）和贪婪策略（需要知道状态价值）\n",
    "def uniform_random_pi(MDP=None, V=None, s=None, a= None):\n",
    "    _, A, _, _, _ = MDP\n",
    "    n = len(A)\n",
    "    return 0 if n == 0 else 1.0/n\n",
    "\n",
    "def greedy_pi(MDP, V, s, a):\n",
    "    S, A, R, P, gamma = MDP\n",
    "    max_v, a_max_v = -float('inf'), []\n",
    "    for a_opt in A:\n",
    "        # 统计后续状态的最大价值以及能达到该状态的行为（可能不止一个）\n",
    "        s_prime, reward, _ = dynamics(s, a_opt)\n",
    "        v_s_prime = get_value(V, s_prime)\n",
    "        if v_s_prime > max_v:\n",
    "            max_v = v_s_prime\n",
    "            a_max_v = [a_opt]\n",
    "        elif v_s_prime == max_v:\n",
    "            a_max_v.append(a_opt)\n",
    "    n = len(a_max_v)\n",
    "    if n == 0: return 0.0\n",
    "    return 1.0/n if a in a_max_v else 0.0\n",
    "\n",
    "def get_pi(Pi, s, a, MDP=None, V=None):\n",
    "    return Pi(MDP, V, s, a)\n",
    "\n",
    "def get_prob(P, s, a, s1):\n",
    "    return P(s, a, s1)\n",
    "\n",
    "def get_reward(R, s, a):\n",
    "    return R(s, a)\n",
    "\n",
    "def set_value(V, s, v):\n",
    "    V[s] = v\n",
    "\n",
    "def get_value(V, s):\n",
    "    return V[s]\n",
    "\n",
    "def display_V(V):\n",
    "    for i in range(16):\n",
    "        print('{0:>6.2f}'.format(V[i]), end='')\n",
    "        if (i + 1) % 4 == 0:\n",
    "            print(\"\")\n",
    "    print()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "def compute_q(MDP, V, s, a):\n",
    "    \"\"\"\n",
    "    根据给定的MDP， 价值函数V，计算状态行为对s，a的价值qsa\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param s:\n",
    "    :param a:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    S, A, R, P, gamma = MDP\n",
    "    q_sa = get_reward(R, s, a)\n",
    "    for s_prime in S:\n",
    "        q_sa += gamma * get_prob(P, s, a, s_prime) * get_value(V, s_prime)\n",
    "    return q_sa\n",
    "\n",
    "def compute_v(MDP, V, Pi, s):\n",
    "    \"\"\"\n",
    "    给定MDP下依据某一策略Pi和当前状态价值函数V计算某状态s的价值\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param Pi:\n",
    "    :param s:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    S, A, R, P, gamma = MDP\n",
    "    v_s = 0\n",
    "    for a in A:\n",
    "        v_s += get_pi(Pi, s, a, MDP, V) * compute_q(MDP, V, s, a)\n",
    "    return v_s\n",
    "\n",
    "def update_V(MDP, V, Pi):\n",
    "    \"\"\"\n",
    "    给定策略和MDP，更新该策略下的价值函数V\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param Pi:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    S, _, _, _, _ = MDP\n",
    "    V_prime = V.copy()\n",
    "    for s in S:\n",
    "        set_value(V_prime, s, compute_v(MDP, V_prime, Pi, s))\n",
    "    return V_prime\n",
    "\n",
    "def policy_evaluate(MDP, V, Pi, n):\n",
    "    \"\"\"\n",
    "    使用n次迭代来评估一个MDP在给定策略Pi下的状态价值，初始值为V\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param Pi:均一随机策略或贪心策略\n",
    "    :param n:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        V = update_V(MDP, V, Pi)\n",
    "    return V\n",
    "\n",
    "def policy_iterate(MDP, V, Pi, n, m):\n",
    "    for i in range(m):\n",
    "        V = policy_evaluate(MDP, V, Pi, n)\n",
    "        Pi = greedy_pi # 第 一 次 迭 代 产 生 新 的 价 值 函 数 后 随 机 使 用 贪 婪 策 略\n",
    "    return V\n",
    "\n",
    "def compute_v_for_max_q(MDP, V, s):\n",
    "    \"\"\"\n",
    "    价值迭代得到最优状态价值过程\n",
    "    根据一个状态下的所有可能的行为价值的最大一个来确定当前状态价值\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param s:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    S, A, R, P, gamma = MDP\n",
    "    v_s = -float('inf')\n",
    "    for a in A:\n",
    "        q_sa = compute_q(MDP, V, s, a)\n",
    "        if q_sa >= v_s:\n",
    "            v_s = q_sa\n",
    "    return v_s\n",
    "\n",
    "def update_V_without_pi(MDP, V):\n",
    "    \"\"\"\n",
    "    在不依赖策略的情况下直接通过后续状态的价值来更新状态价值\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    S, _, _, _, _ = MDP\n",
    "    V_prime = V.copy()\n",
    "    for s in S:\n",
    "        set_value(V_prime, s, compute_v_for_max_q(MDP, V_prime, s))\n",
    "    return V_prime\n",
    "\n",
    "def value_iterate(MDP, V, n):\n",
    "    \"\"\"\n",
    "    价值迭代\n",
    "    :param MDP:\n",
    "    :param V:\n",
    "    :param n:\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    for i in range(n):\n",
    "        V = update_V_without_pi(MDP, V)\n",
    "    return V"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00-14.00-20.00-22.00\n",
      "-14.00-18.00-20.00-20.00\n",
      "-20.00-20.00-18.00-14.00\n",
      "-22.00-20.00-14.00  0.00\n",
      "\n",
      "  0.00 -1.00 -2.00 -3.00\n",
      " -1.00 -2.00 -3.00 -2.00\n",
      " -2.00 -3.00 -2.00 -1.00\n",
      " -3.00 -2.00 -1.00  0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 策略评估\n",
    "V = [0 for i in range(16)]\n",
    "V_pi = policy_evaluate(MDP, V, uniform_random_pi, 100)\n",
    "display_V(V_pi)\n",
    "\n",
    "V = [0 for i in range(16)]\n",
    "V_pi = policy_evaluate(MDP, V, greedy_pi, 100)\n",
    "display_V(V_pi)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.00 -1.00 -2.00 -3.00\n",
      " -1.00 -2.00 -3.00 -2.00\n",
      " -2.00 -3.00 -2.00 -1.00\n",
      " -3.00 -2.00 -1.00  0.00\n",
      "\n",
      "  0.00 -1.00 -2.00 -3.00\n",
      " -1.00 -2.00 -3.00 -2.00\n",
      " -2.00 -3.00 -2.00 -1.00\n",
      " -3.00 -2.00 -1.00  0.00\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 策略迭代\n",
    "V = [0 for _ in range(16)]\n",
    "V_pi = policy_iterate(MDP, V, greedy_pi, 1, 100)\n",
    "display_V(V_pi)\n",
    "\n",
    "# 单纯的价值迭代\n",
    "V_star = [0 for _ in range(16)]\n",
    "V_star = value_iterate(MDP, V, 4)\n",
    "display_V(V_star)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " nesw    w      w      sw   \n",
      "  n      nw    nesw    s    \n",
      "  n     nesw    es     s    \n",
      "  ne     e      e     nesw  \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 观察最优状态下的最优策略\n",
    "def greedy_policy(MDP, V, s):\n",
    "    S, A, R, P, gamma = MDP\n",
    "    max_v, a_max_v = -float('inf'), []\n",
    "    for a_opt in A:\n",
    "        s_prime, reward, _ = dynamics(s, a_opt)\n",
    "        v_s_prime = get_value(V, s_prime)\n",
    "        if v_s_prime > max_v:\n",
    "            max_v = v_s_prime\n",
    "            a_max_v = a_opt\n",
    "        elif v_s_prime == max_v:\n",
    "            a_max_v += a_opt\n",
    "    return str(a_max_v)\n",
    "\n",
    "def display_policy(policy, MDP, V):\n",
    "    S, A, R, P, gamma = MDP\n",
    "    for i in range(16):\n",
    "        print('{0:^6}'.format(policy(MDP, V, S[i])), end=\" \")\n",
    "        if(i+1)%4 == 0:\n",
    "            print(\"\")\n",
    "    print()\n",
    "\n",
    "display_policy(greedy_policy, MDP, V_star)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}